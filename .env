
# Primary LLM Provider for mindmap generation
API_PROVIDER="OLLAMA"

# LightRAG Configuration (following your lightrag_ex.py and lightrag_manager.py)
LLM_MODEL="qwen3-coder:30b"  
EMBEDDING_MODEL="bge-m3:latest"
LLM_HOST="http://100.95.157.120:11434"  # Your specific host from lightrag_manager.py
EMBEDDING_HOST="http://100.95.157.120:11434"
EMBEDDING_DIM="1024"  # Your setting (not 1536) - this must match your embedding model
MAX_EMBED_TOKENS="8192"  # lightrag_ex.py uses "40000" but lightrag_manager.py uses "8192"
TIMEOUT="3000"  # 3000 seconds from your files (not 300)
EMBEDDING_TIMEOUT="6000"  # 6000 seconds from your files (not 600)

# LLM Generation Settings
TEMPERATURE="0.0"  # Temperature for LLM generation (0.0-2.0)
MAX_TOKENS="20000"  # Maximum tokens per LLM response (reduced from 40000 to fix OLLAMA API issues)

# Storage Configuration (following your lightrag_ex.py structure)
WORKING_DIR="./workspace"  # Your exact path from lightrag_ex.py
DOCUMENT_DIR="./workspace/docs"
PROCESSED_DIR="./workspace/processed_docs"  # Following your DOCUMENT_ARCHIVE_DIR pattern
OUTPUT_DIR="./workspace/output"

# PDF Processing
PDF_PROCESSOR="mineru"  # Options: "mineru", "docling", "pymupdf", "pdfplumber"
SKIP_PDF_CONVERSION="false"

# Logging
LOG_LEVEL="INFO"
LOG_DIR="./logs"
LOG_MAX_BYTES="10485760"  # 10MB
LOG_BACKUP_COUNT="5"

# Performance Settings
MAX_CONCURRENT_PROCESSES="4"
CHUNK_SIZE="8192"
MAX_FILE_SIZE_MB="100"
